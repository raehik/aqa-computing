Algorithms
----------

  * **Algorithm:** a sequence of unambiguous instructions for solving a problem
                   in a finite amount of time
  * **Computational complexity:** how economical an algorithm is with *time and
                                  space*
  * **Time complexity:** indicates how fast an algorithm runs
  * **Space complexity:** indicates how much memory an algorithm needs

Things that can impact algorithm run time:

  * Input size (number of inputs)
  * Data type (e.g. integers vs. strings)

In many algorithms, certain inputs can make the algorithm take very long to
return, maybe have to check each individual input value, while others can be
almost instant. The former we call **worst-case complexity**, and the latter
**best-case complexity**. Averaging all the times for each possible input gives
us the **average-case complexity**.

  * **Complexity of a problem:** the *worst-case complexity* of the most
                                 efficient algorithm which solves the problem

When considering the complexity of a problem, we always use the worst-case
value.


### Measuring time

**Basic operation:** the operation in an algorithm contributing the most to
                     total running time

Algorithms **aren't measured in terms of time**, since that would be entirely
dependent on the host computer's speed. Instead, we look at the *number of
operations*. However, this can be demanding to do.

It's sufficient to identify just the operation that contributes most to the
running time: the **basic operation**.


### Order of growth

**Order of growth:** what factor execution time increases when the size of the
                     input is increased

We shouldn't need to know any more, but the book goes into some simple algebra,
proving why doubling the input size of the *natural numbers sum algorithm*
quadruples the execution time. The algorithm is `(n/2)(n + 1)`, so the basic
operation is `(n/2)^2`. We can find the factor change by doing
`time(2n)/time(n)`, and you'll get a `(2n)^2/(n)^2` -> bla bla --> `4/1`.

In the proof, the factor `1/2` was cancelled out. For that reason, they are
ignored in the order of growth. So in this case, the order of growth is
*quadratic*: this is denoted by **O(n^(2))**.


### Big O notation

  * **O(g):** *"big O of g"*, represents the class of functions that grow no
              faster than g
  * **Order of complexity:** the big O complexity of a problem

e.g. an algorithm that grows at rate n^2 cannot belong to the class O(n),
because it grows faster than that.


#### Orders of complexity rankings

**EXAM QUESTION:** You could be asked to rank some given algorithms in terms of
their complexity. Here's the order they go in from least complex to most:

  * O(n^(0)) *{= O(0)}*
  * O(log n)
  * O(n) *(linear time)*
  * O(n log n)
  * O(n^(2)) *(polynomial time)*
  * O(n^(3))
  * ...
  * O(2^(n)) *(exponential time)*
  * O(3^(n))
  * ...
  * O(n!) *(factorial time)*
